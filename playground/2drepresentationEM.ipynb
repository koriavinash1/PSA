{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c508b-26fd-4986-a07c-083e9f3a6580",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a825228-00e3-4171-87ec-8d5d788837a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW, Optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import Callable, Optional, Tuple\n",
    "\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.rcParams.update({'font.size': 1})\n",
    "from matplotlib import rcParams, rcParamsDefault\n",
    "rcParams.update(rcParamsDefault)\n",
    "import sys\n",
    "\n",
    "sys.path.append('../eval')\n",
    "\n",
    "from metrics import slot_mean_corr_coef, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3720e-bc1f-48b5-a0f3-0fb50e0bd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dp = 1000\n",
    "nsamples = 128\n",
    "ncenters = 3\n",
    "nfeatures = 2\n",
    "locs = [[0, 0], [10, 10], [-10, -10], [-10, 10], [10, -10]] #, [-10, 0], [0, 10], [10, 0], [0, -10]]\n",
    "colors = ['green', 'blue', 'orange', 'purple', 'cyan', 'brown', 'pink', 'gray', 'olive', 'red']\n",
    "\n",
    "locs = np.array(locs)\n",
    "nsamples = [nsamples//ncenters, nsamples//ncenters, nsamples - 2*(nsamples//ncenters)]\n",
    "\n",
    "varied = []; cluster_idxs = []\n",
    "idxs = np.arange(len(locs))\n",
    "for _ in range(n_dp):\n",
    "    np.random.shuffle(idxs)\n",
    "    cidx = idxs[:ncenters]\n",
    "\n",
    "    data = []; cluster_idx = []\n",
    "    for ei, i in enumerate(cidx):\n",
    "        samples = np.random.multivariate_normal(mean = locs[i],\n",
    "                                                cov = 0.5*np.eye(nfeatures),\n",
    "                                                size = nsamples[ei])\n",
    "        data.extend(samples)\n",
    "        cluster_idx.extend([i]*nsamples[ei])\n",
    "        \n",
    "    varied.append(data)\n",
    "    cluster_idxs.append(cluster_idx)\n",
    "    \n",
    "    # varied.append(datasets.make_blobs(\n",
    "    #                     n_samples=n_samples,\n",
    "    #                     centers=locs[cidx],\n",
    "    #                     n_features=2)[0])\n",
    "\n",
    "\n",
    "varied = np.array(varied)\n",
    "cluster_idxs = np.array(cluster_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6244449b-e970-4600-aa2b-ad1d548d6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(50,50))\n",
    "for i in range(100):\n",
    "    plt.subplot(10, 10, i+1)\n",
    "    for cidx in np.unique(cluster_idxs[i]):\n",
    "        idx = np.where(cluster_idxs[i] == cidx)[0]\n",
    "        plt.scatter(varied[i][idx, 0], varied[i][idx, 1], color = colors[cidx])\n",
    "        plt.xlim(-15, 15)\n",
    "        plt.ylim(-15, 15)\n",
    "\n",
    "plt.savefig(\"data_samples.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5e34b-968b-42c0-930f-d8cee6a9f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlotAttentionFixedEM(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 2,\n",
    "        num_slots: int = 7,\n",
    "        slot_dim: int = 2,\n",
    "        routing_iters: int = 3,\n",
    "        hidden_dim: int = 2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_slots = num_slots\n",
    "        self.slot_dim = slot_dim\n",
    "        self.routing_iters = routing_iters\n",
    "\n",
    "        self.loc = nn.Parameter(torch.zeros(1, self.slot_dim))\n",
    "        self.logscale = nn.Parameter(torch.zeros(1, self.slot_dim))\n",
    "        self.eps = 1e-6\n",
    "\n",
    "    def forward(self, x: Tensor, num_slots: Optional[int] = None):\n",
    "        # b: batch_size, n: num_inputs, c: input_dim, K: num_slots, d: slot_dim\n",
    "        b, N, d = x.shape\n",
    "        # (b, n, c)\n",
    "        # x = self.ln_inputs(x)\n",
    "\n",
    "        # (b, k, d)\n",
    "        K = num_slots if num_slots is not None else self.num_slots\n",
    "        slots = self.loc + self.logscale.exp() * torch.randn(\n",
    "            b, K, self.slot_dim, device=x.device\n",
    "        )\n",
    "\n",
    "        pi = torch.ones(b, K, 1, device = x.device, dtype = x.dtype)/K\n",
    "        sigma = self.logscale.exp().unsqueeze(1).repeat(1, K, 1)\n",
    "        \n",
    "\n",
    "        for _ in range(self.routing_iters):\n",
    "            slots_prev = slots\n",
    "\n",
    "            # E-step\n",
    "            log_pi    = - 0.5 * torch.tensor(2 * torch.pi, device=x.device).log()\n",
    "            log_scale = - torch.log(torch.clamp(sigma.unsqueeze(2), min = self.eps)) # (B, K, N, d)\n",
    "            exponent  = - 0.5 * (x.unsqueeze(1) - slots_prev.unsqueeze(2)) ** 2 / (sigma.unsqueeze(2)) ** 2 # (B, K, N, d)\n",
    "            log_probs = torch.log(torch.clamp(pi, min = self.eps)) + (exponent + log_pi + log_scale).sum(dim=-1) # (B, K, N)\n",
    "                            \n",
    "            attn = log_probs.softmax(dim=1) + self.eps # (B, K, N)\n",
    "\n",
    "\n",
    "            # M-step\n",
    "            Nk = torch.sum(attn, dim=2, keepdim=True) # (B, K, 1)\n",
    "            pi = Nk / N\n",
    "            \n",
    "            slots = (1 / Nk) * torch.sum(attn.unsqueeze(-1) * x.unsqueeze(1), dim=2) # (B, K, D)\n",
    "\n",
    "            sigma = (1 / Nk) * torch.sum(attn.unsqueeze(-1) * (x.unsqueeze(1) - slots.unsqueeze(2))**2, dim=2) # (B, K, D)\n",
    "            sigma = torch.sqrt(sigma) + self.eps\n",
    "            \n",
    "        return slots, sigma, attn, pi\n",
    "\n",
    "\n",
    "class PositionEmbed(nn.Module):\n",
    "    def __init__(self, out_channels: int = 2, resolution: int = 1500):\n",
    "        super().__init__()\n",
    "        # (1, N, 2)\n",
    "        lp = torch.linspace(0.0, 1.0, steps=resolution).unsqueeze(-1)\n",
    "        self.grid = torch.cat([lp, 1.0 - lp], dim=-1).unsqueeze(0)\n",
    "        self.mlp = nn.Linear(2, out_channels)  # 4 for (x, y, 1-x, 1-y)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        # (1, N, out_channels)\n",
    "        grid = self.mlp(self.grid)\n",
    "        # (batch_size, out_channels, height, width)\n",
    "        return x + grid\n",
    "\n",
    "    def build_grid(self, resolution: Tuple[int, int]) -> Tensor:\n",
    "        xy = [torch.linspace(0.0, 1.0, steps=r) for r in resolution]\n",
    "        xx, yy = torch.meshgrid(xy, indexing=\"ij\")\n",
    "        grid = torch.stack([xx, yy], dim=-1)\n",
    "        grid = grid.unsqueeze(0)\n",
    "        return torch.cat([grid, 1.0 - grid], dim=-1)\n",
    "\n",
    "class SlotAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_inputs: int = 1500,\n",
    "        emb_dim: int = 2,\n",
    "        num_slots: int = 10,\n",
    "        slot_dim: int = 2,\n",
    "        routing_iters: int = 1,\n",
    "        additive_decoder: bool = False,\n",
    "        slot_type: Optional[str] = \"fixedEM\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        enc_act = nn.ReLU()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "            # enc_act,\n",
    "            # PositionEmbed(emb_dim, num_inputs),\n",
    "            # nn.Linear(emb_dim, emb_dim)\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.slot_attention = SlotAttentionFixedEM(\n",
    "            input_dim=emb_dim,\n",
    "            num_slots=num_slots,\n",
    "            slot_dim=slot_dim,\n",
    "            routing_iters=routing_iters,\n",
    "            hidden_dim=emb_dim,\n",
    "        )\n",
    "\n",
    "\n",
    "        dec_act = nn.LeakyReLU()\n",
    "        \n",
    "        self.additive_decoder = additive_decoder\n",
    "        \n",
    "        if self.additive_decoder:\n",
    "            self.decoder = nn.Sequential(\n",
    "                                    # PositionEmbed(slot_dim, num_inputs),\n",
    "                                    # nn.Linear(slot_dim, emb_dim),\n",
    "                                    # dec_act,\n",
    "                                    nn.Linear(emb_dim, emb_dim + 1),\n",
    "                                )\n",
    "\n",
    "        else:\n",
    "            self.decoder = nn.Sequential(\n",
    "                                    # PositionEmbed(slot_dim, num_inputs),\n",
    "                                    # nn.Linear(slot_dim, emb_dim),\n",
    "                                    # dec_act,\n",
    "                                    nn.Linear(emb_dim, emb_dim),\n",
    "                                )\n",
    "            \n",
    "\n",
    "        \n",
    "    def approximate_posterior(self, x: Tensor):\n",
    "        prev_routing_iters = self.slot_attention.routing_iters\n",
    "\n",
    "        self.slot_attention.routing_iters = 10\n",
    "        _, _, _, slots, sigma, _, pi, encodings = self.forward(x)\n",
    "\n",
    "        joint_pi = pi.flatten(0, 1)/slots.shape[0]\n",
    "        joint_sigma = sigma.flatten(0, 1)\n",
    "        joint_slots = slots.flatten(0, 1) \n",
    "        \n",
    "        self.slot_attention.routing_iters = prev_routing_iters\n",
    "        return joint_pi, joint_slots, joint_sigma, encodings.flatten(0, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x: Tensor, use_encoder: bool = True):\n",
    "        # b: batch_size, c: channels, h: height, w: width, d: out_channels\n",
    "        b, n, d = x.shape\n",
    "        # (b, d, h, w)\n",
    "\n",
    "        if use_encoder:\n",
    "            encodings = self.encoder(x)\n",
    "        else:\n",
    "            encodings = x\n",
    "\n",
    "            \n",
    "        # (b, num_slots, slot_dim)\n",
    "        slots, sigma, attn, pi = self.slot_attention(encodings)\n",
    "\n",
    "        x = slots.unsqueeze(1) + (sigma.unsqueeze(1)**0.5)*torch.randn(b, n, slots.shape[1], slots.shape[2])\n",
    "        x = x * pi.unsqueeze(1)\n",
    "        \n",
    "        if self.additive_decoder:\n",
    "            # (b*num_slots, slot_dim, init_h, init_w)\n",
    "            x = x.flatten(0, 1)\n",
    "            \n",
    "            # (b*num_slots, c + 1, h, w)\n",
    "            x = self.decoder(x)\n",
    "    \n",
    "            # (b, num_slots, c + 1, h, w)\n",
    "            x = x.view(b, -1, n, d + 1)\n",
    "            \n",
    "            # (b, num_slots, n, d), (b, num_slots, n, 1)\n",
    "            recons, masks = torch.split(x, [d, 1], dim=3)\n",
    "            masks = masks.softmax(dim=1)\n",
    "            \n",
    "            # (b, c, h, w)\n",
    "            recon_combined = torch.sum(recons * masks, dim=1)\n",
    "        else:\n",
    "            x = x.mean(2)\n",
    "            recon_combined = self.decoder(x)\n",
    "            masks = None; recons = None\n",
    "            \n",
    "        return recon_combined, recons, masks, slots, sigma, attn, pi, encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68d013-1ff3-4245-a185-e6cb4638204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(runid = 1, additive_decoder = True, use_encoder = True):\n",
    "    model = SlotAutoencoder(num_slots = num_slots, \n",
    "                            num_inputs = num_inputs, \n",
    "                            routing_iters = num_inputs,\n",
    "                            additive_decoder = additive_decoder)\n",
    "    optimizer = AdamW(\n",
    "            model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=wd\n",
    "        )\n",
    "    \n",
    "    loss_np = 0\n",
    "    pbar = tqdm(range(nepochs))\n",
    "    data_idx = np.arange(data.shape[0])\n",
    "    \n",
    "    for _  in pbar:\n",
    "        np.random.shuffle(data_idx)    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        x, _, _, slots, sigma, attn, pi, encodings = model(data[data_idx], use_encoder = use_encoder)\n",
    "        loss = F.mse_loss(x, data[data_idx])\n",
    "        \n",
    "        # x = F.sigmoid(x)\n",
    "        # loss = F.binary_cross_entropy(x, data) \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        loss_np = loss.item()\n",
    "        pbar.set_description(\"Training Loss:{:.4f}\".format(loss_np))\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(50,50))\n",
    "    for i in range(100):    \n",
    "        slots_ = slots[i].detach().cpu(); sigma_ = sigma[i].detach().cpu(); pi_ = pi[i].detach().cpu()\n",
    "        samples = encodings[i].detach().cpu().view(-1, 2)\n",
    "        \n",
    "        plt.subplot(10, 10, i+1)\n",
    "        for cidx in np.unique(cluster_idxs[i]):\n",
    "            idx = np.where(cluster_idxs[i] == cidx)[0]\n",
    "            plt.scatter(samples[idx, 0], samples[idx, 1], color = colors[cidx])\n",
    "        \n",
    "        def gaussian_log_prob(x, loc, scale):\n",
    "            return (\n",
    "                -0.5 * torch.tensor(2 * torch.pi, device=x.device).log()\n",
    "                - torch.log(scale)\n",
    "                - 0.5 * (x - loc) ** 2 / scale ** 2\n",
    "            )\n",
    "        \n",
    "        datanp = samples.view(-1, 2)\n",
    "        x, y = np.meshgrid(np.linspace(datanp[:, 0].min(), datanp[:, 0].max(), 100),\n",
    "                           np.linspace(datanp[:, 1].min(), datanp[:, 1].max(), 100))\n",
    "        \n",
    "        coord = np.array([x.ravel(), y.ravel()]).T\n",
    "        coord = torch.from_numpy(coord).unsqueeze(1)\n",
    "        log_probs = torch.log(pi_) + gaussian_log_prob(coord, slots_, sigma_).sum(dim=-1, keepdim=True)\n",
    "        likelihood = log_probs.exp().sum(dim=1).reshape(x.shape)\n",
    "        plt.contour(x, y, likelihood, levels=10)\n",
    "        \n",
    "        plt.grid()\n",
    "        plt.xlabel('Feature-1')\n",
    "        plt.ylabel('Feature-2')\n",
    "        plt.title(f'Datapoint-{i}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"encoded_GMM_fit_run{runid}.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # ================================================================================================\n",
    "    approx_pi, approx_slots, approx_sigma, encodings = model.approximate_posterior(data[:50])\n",
    "    approx_slots = approx_slots.detach().cpu(); approx_sigma = approx_sigma.detach().cpu()\n",
    "    approx_pi = approx_pi.detach().cpu(); approx_samples = encodings.detach().cpu()\n",
    "    \n",
    "    random_samples = np.arange(approx_samples.shape[0])\n",
    "    np.random.shuffle(random_samples)\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    nsamples = -1\n",
    "    \n",
    "    cluster_idx_flat = cluster_idxs.flatten()[random_samples[:nsamples]]\n",
    "    approx_samples = approx_samples[random_samples[:nsamples]]\n",
    "    \n",
    "    for cidx in np.unique(cluster_idx_flat):\n",
    "        idx = np.where(cluster_idx_flat == cidx)[0]\n",
    "        plt.scatter(approx_samples[idx, 0], approx_samples[idx, 1], color = colors[cidx])\n",
    "    \n",
    "    \n",
    "    def gaussian_log_prob(x, loc, scale):\n",
    "        return (\n",
    "            -0.5 * torch.tensor(2 * torch.pi, device=x.device).log()\n",
    "            - torch.log(scale)\n",
    "            - 0.5 * (x - loc) ** 2 / scale ** 2\n",
    "        )\n",
    "    \n",
    "    datanp = approx_samples.view(-1, 2)\n",
    "    x, y = np.meshgrid(np.linspace(datanp[:, 0].min(), datanp[:, 0].max(), 100),\n",
    "                       np.linspace(datanp[:, 1].min(), datanp[:, 1].max(), 100))\n",
    "    \n",
    "    coord = np.array([x.ravel(), y.ravel()]).T\n",
    "    coord = torch.from_numpy(coord).unsqueeze(1)\n",
    "    log_probs = torch.log(approx_pi) + gaussian_log_prob(coord, approx_slots, approx_sigma).sum(dim=-1, keepdim=True)\n",
    "    likelihood = log_probs.exp().sum(dim=1).reshape(x.shape)\n",
    "    plt.contour(x, y, likelihood, levels=10)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.xlabel('Feature-1')\n",
    "    plt.ylabel('Feature-2')\n",
    "    # plt.axis('equal')\n",
    "    plt.title('Aggregate posterior')\n",
    "    \n",
    "    plt.savefig(f\"Aggregate_posterior_run_{runid}.pdf\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return approx_slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd1315-ff4b-40e1-bd4b-3eb52f8fefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.from_numpy(varied).type(torch.float32)\n",
    "data = (data - data.mean(1, keepdim=True))/data.std(1, keepdim=True)\n",
    "# data = (data - data.mean(2, keepdim=True))/data.std(2, keepdim=True)\n",
    "\n",
    "# data = data/data.max(1, keepdim=True)[0]\n",
    "# data = (data - data.min(1, keepdim=True)[0])/(data.max(1, keepdim=True)[0] - data.min(1, keepdim=True)[0] + 1e-5)\n",
    "\n",
    "v_numpy = data[0].detach().cpu().numpy()\n",
    "plt.clf()\n",
    "plt.scatter(v_numpy[:,0], v_numpy[:, 1])\n",
    "# plt.xlim(-1.25, 1.25)\n",
    "# plt.ylim(-1.25, 1.25)\n",
    "plt.show()\n",
    "        \n",
    "nepochs = 15\n",
    "lr = 0.10\n",
    "wd = 1e-6\n",
    "num_slots = 3\n",
    "num_inputs = v_numpy.shape[0] \n",
    "routing_iters = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4814296a-23b3-413f-8ae8-8adc9c89e877",
   "metadata": {},
   "source": [
    "# No encoder, No additive decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db42fa7-ae05-4072-a853-0970d511f7dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs = []\n",
    "for id in range(15):\n",
    "    logs.append(run(id, additive_decoder = False, use_encoder = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9855b50f-9494-4fe0-b25f-b84c0140d05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mccs = []; r2s = []\n",
    "for i in range(len(logs) -1):\n",
    "    xdata = logs[i].view(-1, num_slots, nfeatures) # (logs[i] - logs[i].mean(1, keepdim=True))/logs[i].std(1, keepdim=True)\n",
    "    ydata = logs[i+1].view(-1, num_slots, nfeatures)  #(logs[i+1] - logs[i+1].mean(1, keepdim=True))/logs[i].std(1, keepdim=True)\n",
    "    \n",
    "    mcc_score, ordered_logs = slot_mean_corr_coef(xdata, \n",
    "                                                    ydata,\n",
    "                                                    return_ordered = True,\n",
    "                                                    affine_transformation = True)\n",
    "\n",
    "    ordered_logs = ordered_logs\n",
    "    r2scc = r2_score(xdata, ordered_logs)\n",
    "    \n",
    "    mccs.append(mcc_score.item())\n",
    "    r2s.append(r2scc)\n",
    "\n",
    "\n",
    "\n",
    "idxs = np.argsort(mccs)[::-1]\n",
    "mccs = np.array(mccs)[idxs[:5]]\n",
    "r2s = np.array(r2s)[idxs[:5]]\n",
    "print(f'SMCC: ({np.mean(mccs)} +- {np.std(mccs)}), R2: ({np.mean(r2s)} +- {np.std(r2s)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bc53d-3147-4aaf-a0a3-c35e7874f5d9",
   "metadata": {},
   "source": [
    "# No encoder with additive decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538d92a-c144-4e09-812a-de5e79a67142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs = []\n",
    "for id in range(15 ):\n",
    "    logs.append(run(id, additive_decoder = True, use_encoder = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa4c43-ae12-489f-821a-700cc27aa89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mccs = []; r2s = []\n",
    "for i in range(len(logs) -1):\n",
    "    xdata = logs[i].view(-1, num_slots, nfeatures) # (logs[i] - logs[i].mean(1, keepdim=True))/logs[i].std(1, keepdim=True)\n",
    "    ydata = logs[i+1].view(-1, num_slots, nfeatures)  #(logs[i+1] - logs[i+1].mean(1, keepdim=True))/logs[i].std(1, keepdim=True)\n",
    "    \n",
    "    mcc_score, ordered_logs = slot_mean_corr_coef(xdata, \n",
    "                                                    ydata,\n",
    "                                                    return_ordered = True,\n",
    "                                                    affine_transformation = True)\n",
    "\n",
    "    ordered_logs = ordered_logs\n",
    "    r2scc = r2_score(xdata, ordered_logs)\n",
    "    \n",
    "    mccs.append(mcc_score.item())\n",
    "    r2s.append(r2scc)\n",
    "\n",
    "\n",
    "\n",
    "idxs = np.argsort(mccs)[::-1]\n",
    "mccs = np.array(mccs)[idxs[:5]]\n",
    "r2s = np.array(r2s)[idxs[:5]]\n",
    "print(f'SMCC: ({np.mean(mccs)} +- {np.std(mccs)}), R2: ({np.mean(r2s)} +- {np.std(r2s)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8924f21-772c-4395-98e1-165aa576b8cc",
   "metadata": {},
   "source": [
    "# encoder, No additive decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d6f54f-2e81-48b0-bdd6-27b9cebbeb6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs = []\n",
    "for id in range(15 ):\n",
    "    logs.append(run(id, additive_decoder = False, use_encoder = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a8e37-8bcd-45a3-aa5b-2f2a32d7d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "mccs = []; r2s = []\n",
    "for i in range(len(logs) -1):\n",
    "    xdata = logs[i].view(-1, num_slots, nfeatures) # (logs[i] - logs[i].mean(1, keepdim=True))/logs[i].std(1, keepdim=True)\n",
    "    ydata = logs[i+1].view(-1, num_slots, nfeatures)  #(logs[i+1] - logs[i+1].mean(1, keepdim=True))/logs[i].std(1, keepdim=True)\n",
    "    \n",
    "    mcc_score, ordered_logs = slot_mean_corr_coef(xdata, \n",
    "                                                    ydata,\n",
    "                                                    return_ordered = True,\n",
    "                                                    affine_transformation = True)\n",
    "\n",
    "    ordered_logs = ordered_logs\n",
    "    r2scc = r2_score(xdata, ordered_logs)\n",
    "    \n",
    "    mccs.append(mcc_score.item())\n",
    "    r2s.append(r2scc)\n",
    "\n",
    "\n",
    "\n",
    "idxs = np.argsort(mccs)[::-1]\n",
    "mccs = np.array(mccs)[idxs[:5]]\n",
    "r2s = np.array(r2s)[idxs[:5]]\n",
    "print(f'SMCC: ({np.mean(mccs)} +- {np.std(mccs)}), R2: ({np.mean(r2s)} +- {np.std(r2s)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3f758-17b8-46cf-9d3f-286cd7f4e7f4",
   "metadata": {},
   "source": [
    "# encoder with additive decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401c1b3-a666-4e06-8de3-c50e67783246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logs = []\n",
    "for id in range(15 ):\n",
    "    logs.append(run(id, additive_decoder = True, use_encoder = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67cfd55-87eb-4d2f-99bc-de9aebe7e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "mccs = []; r2s = []\n",
    "for i in range(len(logs) -1):\n",
    "    xdata = logs[i].view(-1, num_slots, nfeatures) # (logs[i] - logs[i].mean(1, keepdim=True))/logs[i].std(1, keepdim=True)\n",
    "    ydata = logs[i+1].view(-1, num_slots, nfeatures)  #(logs[i+1] - logs[i+1].mean(1, keepdim=True))/logs[i].std(1, keepdim=True)\n",
    "    \n",
    "    mcc_score, ordered_logs = slot_mean_corr_coef(xdata, \n",
    "                                                    ydata,\n",
    "                                                    return_ordered = True,\n",
    "                                                    affine_transformation = True)\n",
    "\n",
    "    ordered_logs = ordered_logs\n",
    "    r2scc = r2_score(xdata, ordered_logs)\n",
    "    \n",
    "    mccs.append(mcc_score.item())\n",
    "    r2s.append(r2scc)\n",
    "\n",
    "\n",
    "\n",
    "idxs = np.argsort(mccs)[::-1]\n",
    "mccs = np.array(mccs)[idxs[:5]]\n",
    "r2s = np.array(r2s)[idxs[:5]]\n",
    "print(f'SMCC: ({np.mean(mccs)} +- {np.std(mccs)}), R2: ({np.mean(r2s)} +- {np.std(r2s)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc66ee2f-503f-4b0a-93f4-abf1ba1c4313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
